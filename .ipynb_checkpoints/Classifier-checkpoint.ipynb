{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Feature Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "set_1 = [\"reggae\", \"metal\", \"hiphop\", \"disco\", \"classical\"]\n",
    "set_2 = [\"pop\", \"jazz\", \"country\", \"rock\", \"blues\"]\n",
    "features = [\"spectral_centroid\", \"rolloff\", \"zecr\", \"flux\"]\n",
    "stats = [\"mean\", \"sd\"]\n",
    "frames = [0.5, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mean_s1 = {}\n",
    "dict_sd_s1 = {}\n",
    "\n",
    "for frame in frames:\n",
    "    dict_mean_s1[frame] = {}\n",
    "    dict_mean_s1[frame] = {}\n",
    "    if 'set' not in dict_mean_s1[frame]:\n",
    "        dict_mean_s1[frame]['set'] = ([0] * 100)\n",
    "        dict_mean_s1[frame]['set'].extend([1] * 100)\n",
    "        dict_mean_s1[frame]['set'].extend([2] * 100)\n",
    "        dict_mean_s1[frame]['set'].extend([3] * 100)\n",
    "        dict_mean_s1[frame]['set'].extend([4] * 100)\n",
    "    for f in features:\n",
    "        dict_mean_s1[frame][f] = []\n",
    "        for s in set_1:\n",
    "            filename = s + \"_\" + f + \"_mean_frame_\" + str(int(frame*1000))\n",
    "            # Read set1_data/filename csv with hardcoded column names\n",
    "            df = pd.read_csv(\"set1_data/\" + filename, names=[str(i) for i in range(1, 101)])\n",
    "            # convert to numpy array\n",
    "            df = list(df.values[0])\n",
    "            # append all values to list\n",
    "            dict_mean_s1[frame][f].extend(df)            \n",
    "\n",
    "for frame in frames:\n",
    "    dict_sd_s1[frame] = {}\n",
    "    if 'set' not in dict_sd_s1[frame]:\n",
    "        dict_sd_s1[frame]['set'] = ([0] * 100)\n",
    "        dict_sd_s1[frame]['set'].extend([1] * 100)\n",
    "        dict_sd_s1[frame]['set'].extend([2] * 100)\n",
    "        dict_sd_s1[frame]['set'].extend([3] * 100)\n",
    "        dict_sd_s1[frame]['set'].extend([4] * 100)\n",
    "    # list of 100 0's, 100 1's, 100 2's and 100 3's\n",
    "    for f in features:\n",
    "        dict_sd_s1[frame][f] = []\n",
    "        for s in set_1:\n",
    "            filename = s + \"_\" + f + \"_sd_frame_\" + str(int(frame*1000))\n",
    "            # Read set1_data/filename csv with hardcoded column names\n",
    "            df = pd.read_csv(\"set1_data/\" + filename, names=[str(i) for i in range(1, 101)])\n",
    "            # convert to numpy array\n",
    "            df = list(df.values[0])\n",
    "            # append all values to list\n",
    "            dict_sd_s1[frame][f].extend(df)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mean_s2 = {}\n",
    "dict_sd_s2 = {}\n",
    "\n",
    "for frame in frames:\n",
    "    dict_mean_s2[frame] = {}\n",
    "    dict_mean_s2[frame] = {}\n",
    "    if 'set' not in dict_mean_s2[frame]:\n",
    "        dict_mean_s2[frame]['set'] = ([0] * 100)\n",
    "        dict_mean_s2[frame]['set'].extend([1] * 100)\n",
    "        dict_mean_s2[frame]['set'].extend([2] * 100)\n",
    "        dict_mean_s2[frame]['set'].extend([3] * 100)\n",
    "        dict_mean_s2[frame]['set'].extend([4] * 100)\n",
    "    for f in features:\n",
    "        dict_mean_s2[frame][f] = []\n",
    "        for s in set_2:\n",
    "            filename = s + \"_\" + f + \"_mean_frame_\" + str(int(frame*1000))\n",
    "            # Read set2_data/filename csv with hardcoded column names\n",
    "            df = pd.read_csv(\"set2_data/\" + filename, names=[str(i) for i in range(1, 101)])\n",
    "            # convert to numpy array\n",
    "            df = list(df.values[0])\n",
    "            # append all values to list\n",
    "            dict_mean_s2[frame][f].extend(df)            \n",
    "\n",
    "for frame in frames:\n",
    "    dict_sd_s2[frame] = {}\n",
    "    if 'set' not in dict_sd_s2[frame]:\n",
    "        dict_sd_s2[frame]['set'] = ([0] * 100)\n",
    "        dict_sd_s2[frame]['set'].extend([1] * 100)\n",
    "        dict_sd_s2[frame]['set'].extend([2] * 100)\n",
    "        dict_sd_s2[frame]['set'].extend([3] * 100)\n",
    "        dict_sd_s2[frame]['set'].extend([4] * 100)\n",
    "    # list of 100 0's, 100 1's, 100 2's and 100 3's\n",
    "    for f in features:\n",
    "        dict_sd_s2[frame][f] = []\n",
    "        for s in set_2:\n",
    "            filename = s + \"_\" + f + \"_sd_frame_\" + str(int(frame*1000))\n",
    "            # Read set2_data/filename csv with hardcoded column names\n",
    "            df = pd.read_csv(\"set2_data/\" + filename, names=[str(i) for i in range(1, 101)])\n",
    "            # convert to numpy array\n",
    "            df = list(df.values[0])\n",
    "            # append all values to list\n",
    "            dict_sd_s2[frame][f].extend(df)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_05 = pd.DataFrame.from_dict(dict_mean_s2[0.5])\n",
    "df_mean_1 = pd.DataFrame.from_dict(dict_mean_s2[1])\n",
    "df_mean_3 = pd.DataFrame.from_dict(dict_mean_s2[3])\n",
    "df_sd_05 = pd.DataFrame.from_dict(dict_sd_s2[0.5])\n",
    "df_sd_1 = pd.DataFrame.from_dict(dict_sd_s2[1])\n",
    "df_sd_3 = pd.DataFrame.from_dict(dict_sd_s2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mean_05 = pd.DataFrame.from_dict(dict_mean_s1[0.5])\n",
    "df1_mean_1 = pd.DataFrame.from_dict(dict_mean_s1[1])\n",
    "df1_mean_3 = pd.DataFrame.from_dict(dict_mean_s1[3])\n",
    "df1_sd_05 = pd.DataFrame.from_dict(dict_sd_s1[0.5])\n",
    "df1_sd_1 = pd.DataFrame.from_dict(dict_sd_s1[1])\n",
    "df1_sd_3 = pd.DataFrame.from_dict(dict_sd_s1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mean_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mean_05 has 100 rows and 5 columns\n",
    "# Run classification taking 2 features at a time\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create all possible combinations of features\n",
    "flist = []\n",
    "for i in range(1, len(features)+1):\n",
    "    for subset in itertools.combinations(features, i):\n",
    "        flist.append(list(subset))\n",
    "\n",
    "def run_knn(df):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('set', axis=1), df['set'], test_size=0.2, random_state=42)\n",
    "\n",
    "    for f_columns in flist:\n",
    "        # Create KNN classifier\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train[f_columns], y_train)\n",
    "\n",
    "        # Predict the response for test dataset\n",
    "        y_pred = knn.predict(X_test[f_columns])\n",
    "\n",
    "        # Model Accuracy, how often is the classifier correct?\n",
    "        # print features used and accuracy\n",
    "        print(\"Accuracy: \", \"{:.2f}\".format(accuracy_score(y_test, y_pred)), \", KNN: Features: \", f_columns, \", Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # print(\"Accuracy:\", accuracy_score(y_test[f_columns], y_pred))\n",
    "\n",
    "def run_svc(df):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('set', axis=1), df['set'], test_size=0.2, random_state=42)\n",
    "\n",
    "    for f_columns in flist:\n",
    "        # Create svc classifier\n",
    "        svc = SVC(kernel='linear', C = 1.0) \n",
    "        svc.fit(X_train[f_columns], y_train)\n",
    "\n",
    "        # Predict the response for test dataset\n",
    "        y_pred = svc.predict(X_test[f_columns])\n",
    "        print(\"Accuracy: \", \"{:.2f}\".format(accuracy_score(y_test, y_pred)), \", SVC: Features: \", f_columns)\n",
    "\n",
    "\n",
    "def run_randomforest(df):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('set', axis=1), df['set'], test_size=0.2, random_state=42)\n",
    "\n",
    "    for f_columns in flist:\n",
    "        # Create random forest classifier\n",
    "        rf = RandomForestClassifier(n_estimators=100)\n",
    "        rf.fit(X_train[f_columns], y_train)\n",
    "\n",
    "        # Predict the response for test dataset\n",
    "        y_pred = rf.predict(X_test[f_columns])\n",
    "        print(\"Accuracy: \", \"{:.2f}\".format(accuracy_score(y_test, y_pred)), \", Random Forest: Features: \", f_columns)\n",
    "\n",
    "def run_mlp(df):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('set', axis=1), df['set'], test_size=0.2, random_state=42)\n",
    "\n",
    "    for f_columns in flist:\n",
    "        # Create MLP classifier\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "        mlp.fit(X_train[f_columns], y_train)\n",
    "\n",
    "        # Predict the response for test dataset\n",
    "        y_pred = mlp.predict(X_test[f_columns])\n",
    "        print(\"Accuracy: \", \"{:.2f}\".format(accuracy_score(y_test, y_pred)), \", MLP: Features: \", f_columns)\n",
    "\n",
    "def run_nb(df):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('set', axis=1), df['set'], test_size=0.2, random_state=42)\n",
    "\n",
    "    for f_columns in flist:\n",
    "        # Create Naive Bayes classifier\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(X_train[f_columns], y_train)\n",
    "\n",
    "        # Predict the response for test dataset\n",
    "        y_pred = nb.predict(X_test[f_columns])\n",
    "        print(\"Accuracy: \", \"{:.2f}\".format(accuracy_score(y_test, y_pred)), \", Naive Bayes: Features: \", f_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_knn(df_mean_05)\n",
    "# run_svc(df_mean_05)\n",
    "# run_randomforest(df_mean_05)\n",
    "# run_mlp(df_mean_05)\n",
    "# run_nb(df_mean_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 0.5 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df1_mean_05)\n",
    "run_mlp(df1_mean_05)\n",
    "run_randomforest(df1_mean_05)\n",
    "run_nb(df1_mean_05)\n",
    "run_nb(df1_mean_05)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df1_sd_05)\n",
    "run_mlp(df1_sd_05)\n",
    "run_randomforest(df1_sd_05)\n",
    "run_nb(df1_sd_05)\n",
    "run_nb(df1_sd_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With 1 Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df1_mean_1)\n",
    "run_mlp(df1_mean_1)\n",
    "run_randomforest(df1_mean_1)\n",
    "run_nb(df1_mean_1)\n",
    "run_nb(df1_mean_1)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df1_sd_1)\n",
    "run_mlp(df1_sd_1)\n",
    "run_randomforest(df1_sd_1)\n",
    "run_nb(df1_sd_1)\n",
    "run_nb(df1_sd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 3 Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df1_mean_3)\n",
    "run_mlp(df1_mean_3)\n",
    "run_randomforest(df1_mean_3)\n",
    "run_nb(df1_mean_3)\n",
    "run_nb(df1_mean_3)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df1_sd_3)\n",
    "run_mlp(df1_sd_3)\n",
    "run_randomforest(df1_sd_3)\n",
    "run_nb(df1_sd_3)\n",
    "run_nb(df1_sd_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 0.5 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df_mean_05)\n",
    "run_mlp(df_mean_05)\n",
    "run_randomforest(df_mean_05)\n",
    "run_nb(df_mean_05)\n",
    "run_nb(df_mean_05)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df_sd_05)\n",
    "run_mlp(df_sd_05)\n",
    "run_randomforest(df_sd_05)\n",
    "run_nb(df_sd_05)\n",
    "run_nb(df_sd_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With 1 Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df_mean_1)\n",
    "run_mlp(df_mean_1)\n",
    "run_randomforest(df_mean_1)\n",
    "run_nb(df_mean_1)\n",
    "run_nb(df_mean_1)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df_sd_1)\n",
    "run_mlp(df_sd_1)\n",
    "run_randomforest(df_sd_1)\n",
    "run_nb(df_sd_1)\n",
    "run_nb(df_sd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 3 Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "run_nb(df_mean_3)\n",
    "run_mlp(df_mean_3)\n",
    "run_randomforest(df_mean_3)\n",
    "run_nb(df_mean_3)\n",
    "run_nb(df_mean_3)\n",
    "\n",
    "print(\"SD\")\n",
    "run_nb(df_sd_3)\n",
    "run_mlp(df_sd_3)\n",
    "run_randomforest(df_sd_3)\n",
    "run_nb(df_sd_3)\n",
    "run_nb(df_sd_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
